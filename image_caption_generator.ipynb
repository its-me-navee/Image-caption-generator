{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np\n",
    "# data processing, CSV file I / O (e.g. pd.read_csv)\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\n",
    "from keras.layers import concatenate, BatchNormalization, Input\n",
    "from keras.layers import add\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "import matplotlib.pyplot as plt # for plotting data\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_description(text):\n",
    "\tmapping = dict()\n",
    "\tfor line in text.split(\"\\n\"):\n",
    "\t\ttoken = line.split(\"\\t\")\n",
    "\t\tif len(line) < 2: # remove short descriptions\n",
    "\t\t\tcontinue\n",
    "\t\timg_id = token[0].split('.')[0] # name of the image\n",
    "\t\timg_des = token[1]\t\t\t # description of the image\n",
    "\t\tif img_id not in mapping:\n",
    "\t\t\tmapping[img_id] = list()\n",
    "\t\tmapping[img_id].append(img_des)\n",
    "\treturn mapping\n",
    "token_path = 'image_dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\n",
    "text = open(token_path, 'r', encoding = 'utf-8').read()\n",
    "descriptions = load_description(text)\n",
    "print(descriptions['1000268201_693b08cb0e'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_description(desc):\n",
    "\tfor key, des_list in desc.items():\n",
    "\t\tfor i in range(len(des_list)):\n",
    "\t\t\tcaption = des_list[i]\n",
    "\t\t\tcaption = [ch for ch in caption if ch not in string.punctuation]\n",
    "\t\t\tcaption = ''.join(caption)\n",
    "\t\t\tcaption = caption.split(' ')\n",
    "\t\t\tcaption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]\n",
    "\t\t\tcaption = ' '.join(caption)\n",
    "\t\t\tdes_list[i] = caption\n",
    "\n",
    "clean_description(descriptions)\n",
    "descriptions['1000268201_693b08cb0e']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(desc):\n",
    "\twords = set()\n",
    "\tfor key in desc.keys():\n",
    "\t\tfor line in desc[key]:\n",
    "\t\t\twords.update(line.split())\n",
    "\treturn words\n",
    "vocab = to_vocab(descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images = '/kaggle / input / flickr8k / flickr_data / Flickr_Data / Images/'\n",
    "# Create a list of all image names in the directory\n",
    "img = glob.glob(images + '*.jpg')\n",
    "train_path = 'image_dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
    "train_images = open(train_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\n",
    "train_img = [] # list of all images in training set\n",
    "for im in img:\n",
    "\tif(im[len(images):] in train_images):\n",
    "\t\ttrain_img.append(im)\n",
    "\t\t\n",
    "# load descriptions of training set in a dictionary. Name of the image will act as ey\n",
    "def load_clean_descriptions(des, dataset):\n",
    "\tdataset_des = dict()\n",
    "\tfor key, des_list in des.items():\n",
    "\t\tif key+'.jpg' in dataset:\n",
    "\t\t\tif key not in dataset_des:\n",
    "\t\t\t\tdataset_des[key] = list()\n",
    "\t\t\tfor line in des_list:\n",
    "\t\t\t\tdesc = 'startseq ' + line + ' endseq'\n",
    "\t\t\t\tdataset_des[key].append(desc)\n",
    "\treturn dataset_des\n",
    "\n",
    "train_descriptions = load_clean_descriptions(descriptions, train_images)\n",
    "print(train_descriptions['1000268201_693b08cb0e'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "def preprocess_img(img_path):\n",
    "\t# inception v3 excepts img in 299 * 299 * 3\n",
    "\timg = load_img(img_path, target_size = (299, 299))\n",
    "\tx = img_to_array(img)\n",
    "\t# Add one more dimension\n",
    "\tx = np.expand_dims(x, axis = 0)\n",
    "\tx = preprocess_input(x)\n",
    "\treturn x\n",
    "\n",
    "def encode(image):\n",
    "\timage = preprocess_img(image)\n",
    "\tvec = model.predict(image)\n",
    "\tvec = np.reshape(vec, (vec.shape[1]))\n",
    "\treturn vec\n",
    "\n",
    "base_model = InceptionV3(weights = 'imagenet')\n",
    "model = Model(base_model.input, base_model.layers[-2].output)\n",
    "# run the encode function on all train images and store the feature vectors in a list\n",
    "encoding_train = {}\n",
    "for img in train_img:\n",
    "\tencoding_train[img[len(images):]] = encode(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all training captions\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "\tfor caption in val:\n",
    "\t\tall_train_captions.append(caption)\n",
    "\n",
    "# consider only words which occur atleast 10 times\n",
    "vocabulary = vocab\n",
    "threshold = 10 # you can change this value according to your need\n",
    "word_counts = {}\n",
    "for cap in all_train_captions:\n",
    "\tfor word in cap.split(' '):\n",
    "\t\tword_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "vocab = [word for word in word_counts if word_counts[word] >= threshold]\n",
    "\n",
    "# word mapping to integers\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "\n",
    "ix = 1\n",
    "for word in vocab:\n",
    "\twordtoix[word] = ix\n",
    "\tixtoword[ix] = word\n",
    "\tix += 1\n",
    "\t\n",
    "# find the maximum length of a description in a dataset\n",
    "max_length = max(len(des.split()) for des in all_train_captions)\n",
    "max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, y = list(), list(), list()\n",
    "for key, des_list in train_descriptions.items():\n",
    "\tpic = train_features[key + '.jpg']\n",
    "\tfor cap in des_list:\n",
    "\t\tseq = [wordtoix[word] for word in cap.split(' ') if word in wordtoix]\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(pic)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\n",
    "X2 = np.array(X2)\n",
    "X1 = np.array(X1)\n",
    "y = np.array(y)\n",
    "\n",
    "# load glove vectors for embedding layer\n",
    "embeddings_index = {}\n",
    "golve_path ='/kaggle / input / glove-global-vectors-for-word-representation / glove.6B.200d.txt'\n",
    "glove = open(golve_path, 'r', encoding = 'utf-8').read()\n",
    "for line in glove.split(\"\\n\"):\n",
    "\tvalues = line.split(\" \")\n",
    "\tword = values[0]\n",
    "\tindices = np.asarray(values[1: ], dtype = 'float32')\n",
    "\tembeddings_index[word] = indices\n",
    "\n",
    "emb_dim = 200\n",
    "emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "for word, i in wordtoix.items():\n",
    "\temb_vec = embeddings_index.get(word)\n",
    "\tif emb_vec is not None:\n",
    "\t\temb_matrix[i] = emb_vec\n",
    "emb_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "ip1 = Input(shape = (2048, ))\n",
    "fe1 = Dropout(0.2)(ip1)\n",
    "fe2 = Dense(256, activation = 'relu')(fe1)\n",
    "ip2 = Input(shape = (max_length, ))\n",
    "se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(ip2)\n",
    "se2 = Dropout(0.2)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation = 'relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "model = Model(inputs = [ip1, ip2], outputs = outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([emb_matrix])\n",
    "model.layers[2].trainable = False\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "model.fit([X1, X2], y, epochs = 50, batch_size = 256)\n",
    "# you can increase the number of epochs for better results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(pic):\n",
    "\tstart = 'startseq'\n",
    "\tfor i in range(max_length):\n",
    "\t\tseq = [wordtoix[word] for word in start.split() if word in wordtoix]\n",
    "\t\tseq = pad_sequences([seq], maxlen = max_length)\n",
    "\t\tyhat = model.predict([pic, seq])\n",
    "\t\tyhat = np.argmax(yhat)\n",
    "\t\tword = ixtoword[yhat]\n",
    "\t\tstart += ' ' + word\n",
    "\t\tif word == 'endseq':\n",
    "\t\t\tbreak\n",
    "\tfinal = start.split()\n",
    "\tfinal = final[1:-1]\n",
    "\tfinal = ' '.join(final)\n",
    "\treturn final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
